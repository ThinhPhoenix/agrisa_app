import { useAgrisaColors } from "@/domains/agrisa_theme/hooks/useAgrisaColor";
import { useAuthStore } from "@/domains/auth/stores/auth.store";
import {
  Box,
  Button,
  ButtonIcon,
  ButtonText,
  HStack,
  Spinner,
  Text,
  VStack,
} from "@gluestack-ui/themed";
import {
  cacheDirectory,
  copyAsync,
  deleteAsync,
  documentDirectory,
} from "expo-file-system/legacy";
import { useRouter } from "expo-router";
import { Camera, CheckCircle2, ScanFace, Video, X } from "lucide-react-native";
import React, { useCallback, useEffect, useMemo, useRef, useState } from "react";
import { Alert, Dimensions, Platform, StyleSheet } from "react-native";
import Svg, { Defs, Ellipse, Mask, Rect } from "react-native-svg";
import {
  Camera as VisionCamera,
  useCameraDevice,
  useCameraPermission,
} from "react-native-vision-camera";
import {
  Face,
  Camera as FaceCamera,
  FaceDetectionOptions,
} from "react-native-vision-camera-face-detector";
import { useEkyc } from "../hooks/use-ekyc";
import { useEkycStore } from "../stores/ekyc.store";
import { useSafeAreaInsets } from "react-native-safe-area-context";

const { width: SCREEN_WIDTH, height: SCREEN_HEIGHT } = Dimensions.get("window");
// ‚úÖ Constants - TƒÉng k√≠ch th∆∞·ªõc khung face
const FACE_OVAL_WIDTH = SCREEN_WIDTH * 0.75; // TƒÉng t·ª´ 0.65 l√™n 0.75
const FACE_OVAL_HEIGHT = FACE_OVAL_WIDTH * 1.35; // Gi·∫£m t·ª∑ l·ªá t·ª´ 1.4 xu·ªëng 1.35 ƒë·ªÉ c√¢n ƒë·ªëi h∆°n

const RECORDING_DURATION = 10000; // 10 gi√¢y
const NO_FACE_TIMEOUT = 120000; // 2 ph√∫t
const FACE_DETECTION_INTERVAL = 500; // Ki·ªÉm tra m·ªói 500ms
const MAX_NO_FACE_PAUSE = 3000; // C·∫£nh b√°o sau 3 gi√¢y kh√¥ng c√≥ face

// Loading ring constants
const RING_STROKE_WIDTH = 6;
const RING_GAP = 12; // Gi·∫£m gap t·ª´ 15 xu·ªëng 12

type ScanStep =
| "instruction"
| "preparing"
| "recording"
| "processing"
| "success";

export const FaceScanScreen = () => {
  const insets = useSafeAreaInsets();
  const CAMERA_HEIGHT = useMemo(() => {
    return SCREEN_HEIGHT - insets.top - insets.bottom;
  }, [insets.top, insets.bottom]);
  const router = useRouter();
  const { colors } = useAgrisaColors();
  const { faceScanMutation } = useEkyc();
  const { ocrData } = useEkycStore();
  const { user } = useAuthStore();

  // Camera hooks
  const { hasPermission, requestPermission } = useCameraPermission();
  const device = useCameraDevice("front");

  // Refs
  const cameraRef = useRef<VisionCamera>(null);
  const recordingTimeRef = useRef<number>(0);
  const pausedTimeRef = useRef<number>(0);
  const lastFaceDetectedTimeRef = useRef<number>(Date.now());
  const faceDetectionCallCountRef = useRef<number>(0);
  const isFaceDetectionWorkingRef = useRef<boolean>(false);
  const isRecordingRef = useRef<boolean>(false);
  const recordedVideoPathRef = useRef<string | null>(null);
  const currentlyPausedRef = useRef<boolean>(false);

  // Timeout refs
  const noFaceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const recordingIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const faceDetectionCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const pauseCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // States
  const [currentStep, setCurrentStep] = useState<ScanStep>("instruction");
  const [faceDetected, setFaceDetected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [recordingProgress, setRecordingProgress] = useState(0);
  const [faceDetectionStatus, setFaceDetectionStatus] = useState<
    "working" | "error" | "checking"
  >("checking");

  // Face Detection Options
  const faceDetectionOptions = useMemo<FaceDetectionOptions>(
    () => ({
      performanceMode: "accurate",
      landmarkMode: "all",
      contourMode: "none",
      classificationMode: "all",
      trackingEnabled: true,
      windowWidth: SCREEN_WIDTH,
      windowHeight: CAMERA_HEIGHT,
    }),
    [CAMERA_HEIGHT]
  );

  // ==================== CLEANUP FUNCTIONS ====================

  const cleanupRecording = useCallback(() => {
    console.log("üßπ Cleaning up recording...");

    if (recordingIntervalRef.current) {
      clearInterval(recordingIntervalRef.current);
      recordingIntervalRef.current = null;
    }
    if (noFaceTimeoutRef.current) {
      clearTimeout(noFaceTimeoutRef.current);
      noFaceTimeoutRef.current = null;
    }
    if (faceDetectionCheckIntervalRef.current) {
      clearInterval(faceDetectionCheckIntervalRef.current);
      faceDetectionCheckIntervalRef.current = null;
    }
    if (pauseCheckIntervalRef.current) {
      clearInterval(pauseCheckIntervalRef.current);
      pauseCheckIntervalRef.current = null;
    }

    recordingTimeRef.current = 0;
    pausedTimeRef.current = 0;
    isRecordingRef.current = false;
    recordedVideoPathRef.current = null;
    isFaceDetectionWorkingRef.current = false;
    faceDetectionCallCountRef.current = 0;
    currentlyPausedRef.current = false;
  }, []);

  const resetToInitialState = useCallback(() => {
    console.log("üîÑ Resetting to initial state...");
    cleanupRecording();
    setCurrentStep("instruction");
    setFaceDetected(false);
    setIsRecording(false);
    setIsPaused(false);
    setRecordingProgress(0);
    setFaceDetectionStatus("checking");
  }, [cleanupRecording]);

  // ==================== FACE DETECTION HEALTH CHECK ====================

  const startFaceDetectionCheck = useCallback(() => {
    console.log("üîç Starting face detection health check...");

    if (faceDetectionCheckIntervalRef.current) {
      clearInterval(faceDetectionCheckIntervalRef.current);
    }

    let checkCount = 0;
    const maxChecks = 10;

    faceDetectionCheckIntervalRef.current = setInterval(() => {
      checkCount++;
      console.log(
        `üîç Check ${checkCount}/${maxChecks}, callbacks: ${faceDetectionCallCountRef.current}`
      );

      if (faceDetectionCallCountRef.current >= 3) {
        console.log("‚úÖ Face detection is working!");
        isFaceDetectionWorkingRef.current = true;
        setFaceDetectionStatus("working");

        if (faceDetectionCheckIntervalRef.current) {
          clearInterval(faceDetectionCheckIntervalRef.current);
          faceDetectionCheckIntervalRef.current = null;
        }
        return;
      }

      if (checkCount >= maxChecks) {
        console.warn("‚ö†Ô∏è Face detection NOT working! Using auto mode.");
        isFaceDetectionWorkingRef.current = false;
        setFaceDetectionStatus("error");

        if (faceDetectionCheckIntervalRef.current) {
          clearInterval(faceDetectionCheckIntervalRef.current);
          faceDetectionCheckIntervalRef.current = null;
        }

        Alert.alert(
          "Th√¥ng b√°o",
          "Kh√¥ng th·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng ph√°t hi·ªán khu√¥n m·∫∑t. H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông ghi h√¨nh 10 gi√¢y.\n\nVui l√≤ng gi·ªØ khu√¥n m·∫∑t trong khung trong su·ªët qu√° tr√¨nh.",
          [
            {
              text: "Ti·∫øp t·ª•c",
              onPress: () => {
                setTimeout(() => {
                  startRecording();
                }, 1000);
              },
            },
            {
              text: "H·ªßy",
              style: "cancel",
              onPress: () => resetToInitialState(),
            },
          ]
        );
      }
    }, FACE_DETECTION_INTERVAL);
  }, [resetToInitialState]);

  const handleFacesDetection = useCallback(
    (faces: Face[]) => {
      try {
        faceDetectionCallCountRef.current++;

        const hasValidFace =
          faces?.length > 0 &&
          (() => {
            const face = faces[0];
            return (
              face.leftEyeOpenProbability > 0.7 &&
              face.rightEyeOpenProbability > 0.7 &&
              Math.abs(face.yawAngle) < 15 &&
              Math.abs(face.pitchAngle) < 15
            );
          })();

        setFaceDetected(hasValidFace);

        if (currentStep === "recording" && isFaceDetectionWorkingRef.current) {
          if (hasValidFace) {
            lastFaceDetectedTimeRef.current = Date.now();

            if (currentlyPausedRef.current) {
              console.log("üòä Face detected, RESUMING...");
              currentlyPausedRef.current = false;
              setIsPaused(false);
              pausedTimeRef.current = 0;
            }
          } else {
            if (!currentlyPausedRef.current) {
              console.log("üòü No face detected, PAUSING...");
              currentlyPausedRef.current = true;
              setIsPaused(true);
            }
          }
        } else if (
          currentStep === "preparing" &&
          hasValidFace &&
          isFaceDetectionWorkingRef.current
        ) {
          console.log("üë§ Face detected in preparing, starting recording...");
          startRecording();
        }
      } catch (error) {
        console.error("‚ùå Face detection error:", error);
        setFaceDetected(false);
      }
    },
    [currentStep]
  );

  // ==================== RECORDING FUNCTIONS ====================

  const startPreparation = useCallback(() => {
    console.log("üé¨ Starting preparation...");

    cleanupRecording();
    setFaceDetected(false);
    setIsRecording(false);
    setIsPaused(false);
    setRecordingProgress(0);
    setFaceDetectionStatus("checking");
    setCurrentStep("preparing");
    startFaceDetectionCheck();

    noFaceTimeoutRef.current = setTimeout(() => {
      console.error("‚è∞ No face timeout");
      stopRecording();
      Alert.alert(
        "H·∫øt th·ªùi gian",
        "Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t. Vui l√≤ng th·ª≠ l·∫°i.",
        [
          {
            text: "OK",
            onPress: () => {
              if (isRecordingRef.current) {
                stopRecording();
              }
              resetToInitialState();
            },
          },
        ]
      );
    }, NO_FACE_TIMEOUT);
  }, [cleanupRecording, startFaceDetectionCheck, resetToInitialState]);

  const startRecording = useCallback(async () => {
    if (!cameraRef.current || isRecordingRef.current) {
      console.log("‚ö†Ô∏è Cannot start recording");
      return;
    }

    try {
      console.log("üé• Starting recording...");

      setCurrentStep("recording");
      setIsRecording(true);
      isRecordingRef.current = true;
      currentlyPausedRef.current = false;
      setIsPaused(false);
      recordingTimeRef.current = 0;
      pausedTimeRef.current = 0;
      setRecordingProgress(0);

      await cameraRef.current.startRecording({
        onRecordingFinished: (video) => {
          console.log("‚úÖ Recording finished:", video.path);
          recordedVideoPathRef.current = video.path;
          isRecordingRef.current = false;
          setIsRecording(false);

          if (recordingTimeRef.current >= RECORDING_DURATION) {
            handleSubmit(video.path);
          }
        },
        onRecordingError: (error) => {
          console.error("‚ùå Recording error:", error);
          isRecordingRef.current = false;
          setIsRecording(false);
          stopRecording();

          Alert.alert("L·ªói", "Kh√¥ng th·ªÉ ghi h√¨nh. Vui l√≤ng th·ª≠ l·∫°i.", [
            {
              text: "Th·ª≠ l·∫°i",
              onPress: () => resetToInitialState(),
            },
          ]);
        },
      });

      if (recordingIntervalRef.current) {
        clearInterval(recordingIntervalRef.current);
      }

      recordingIntervalRef.current = setInterval(() => {
        const shouldPause =
          isFaceDetectionWorkingRef.current && currentlyPausedRef.current;

        if (!shouldPause) {
          recordingTimeRef.current += 100;

          const progress = Math.min(
            (recordingTimeRef.current / RECORDING_DURATION) * 100,
            100
          );
          setRecordingProgress(progress);

          console.log(
            `üìä Recording: ${recordingTimeRef.current}ms (${progress.toFixed(1)}%) - Paused: ${shouldPause}`
          );

          if (recordingTimeRef.current >= RECORDING_DURATION) {
            console.log("‚úÖ Recording duration reached");
            stopRecording();
          }
        } else {
          pausedTimeRef.current += 100;
          console.log(`‚è∏Ô∏è Paused: ${pausedTimeRef.current}ms`);
        }
      }, 100);

      if (isFaceDetectionWorkingRef.current) {
        if (pauseCheckIntervalRef.current) {
          clearInterval(pauseCheckIntervalRef.current);
        }

        pauseCheckIntervalRef.current = setInterval(() => {
          if (currentlyPausedRef.current) {
            const timeSinceLastFace =
              Date.now() - lastFaceDetectedTimeRef.current;
              
              if (timeSinceLastFace > MAX_NO_FACE_PAUSE) {
                console.warn("‚ö†Ô∏è Paused too long");
                stopRecording();

              if (pauseCheckIntervalRef.current) {
                clearInterval(pauseCheckIntervalRef.current);
                pauseCheckIntervalRef.current = null;
                stopRecording();
              }
              
              Alert.alert(
                "C·∫£nh b√°o",
                "Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t. Vui l√≤ng ƒë∆∞a khu√¥n m·∫∑t v√†o khung.",
                [
                  {
                    text: "Ti·∫øp t·ª•c",
                    onPress: () => {
                      lastFaceDetectedTimeRef.current = Date.now();
                      pausedTimeRef.current = 0;
                      startRecording();
                    },
                  },
                  {
                    text: "B·∫Øt ƒë·∫ßu l·∫°i",
                    onPress: () => {
                      stopRecording();
                      resetToInitialState();
                    },
                  },
                ]
              );
            }
          }
        }, 1000);
      }
    } catch (error) {
      console.error("‚ùå Start recording error:", error);
      isRecordingRef.current = false;
      setIsRecording(false);
      stopRecording();

      Alert.alert("L·ªói", "Kh√¥ng th·ªÉ b·∫Øt ƒë·∫ßu ghi h√¨nh. Vui l√≤ng th·ª≠ l·∫°i.", [
        {
          text: "Th·ª≠ l·∫°i",
          onPress: () => resetToInitialState(),
        },
      ]);
    }
  }, [resetToInitialState]);

  const stopRecording = useCallback(async () => {
    if (!cameraRef.current || !isRecordingRef.current) {
      console.log("‚ö†Ô∏è Already stopped or not recording");
      return;
    }

    try {
      console.log("üõë Stopping recording...");

      if (recordingIntervalRef.current) {
        clearInterval(recordingIntervalRef.current);
        recordingIntervalRef.current = null;
      }
      if (noFaceTimeoutRef.current) {
        clearTimeout(noFaceTimeoutRef.current);
        noFaceTimeoutRef.current = null;
      }
      if (pauseCheckIntervalRef.current) {
        clearInterval(pauseCheckIntervalRef.current);
        pauseCheckIntervalRef.current = null;
      }

      await cameraRef.current.stopRecording();
      console.log("‚úÖ Recording stopped");
    } catch (error) {
      console.error("‚ùå Stop recording error:", error);
      isRecordingRef.current = false;
      setIsRecording(false);
    }
  }, []);

  // ==================== SUBMIT ====================

  const handleSubmit = useCallback(
    async (videoPath: string) => {
      console.log("üì§ Submitting video...");
      console.log("Original video path:", videoPath);

      if (!user?.id || !ocrData.cccd_front) {
        stopRecording();

        Alert.alert(
          "L·ªói",
          "Thi·∫øu th√¥ng tin x√°c th·ª±c. Vui l√≤ng quay l·∫°i v√† th·ª±c hi·ªán l·∫°i t·ª´ ƒë·∫ßu.",
          [{ text: "ƒê√≥ng", onPress: () => router.back() }]
        );
        return;
      }

      setCurrentStep("processing");

      try {
        const formData = new FormData();
        formData.append("user_id", user.id.toString());

        let finalVideoUri = videoPath;

        if (Platform.OS === "android") {
          const fileName = `face_scan_${Date.now()}.mp4`;

          if (!documentDirectory) {
            console.warn("‚ö†Ô∏è documentDirectory not available, using cache");
            const newPath = `${cacheDirectory}${fileName}`;

            try {
              await copyAsync({
                from: videoPath,
                to: newPath,
              });

              console.log("‚úÖ Video copied to cache:", newPath);
              finalVideoUri = newPath;
            } catch (copyError) {
              console.error("‚ùå Error copying video:", copyError);
              finalVideoUri = videoPath.startsWith("file://")
                ? videoPath
                : `file://${videoPath}`;
            }
          } else {
            const newPath = `${documentDirectory}${fileName}`;

            try {
              await copyAsync({
                from: videoPath,
                to: newPath,
              });

              console.log("‚úÖ Video copied to:", newPath);
              finalVideoUri = newPath;
            } catch (copyError) {
              console.error("‚ùå Error copying video:", copyError);
              finalVideoUri = videoPath.startsWith("file://")
                ? videoPath
                : `file://${videoPath}`;
            }
          }
        } else {
          finalVideoUri = videoPath.startsWith("file://")
            ? videoPath
            : `file://${videoPath}`;
        }

        console.log("Final video URI:", finalVideoUri);

        formData.append("video", {
          uri: finalVideoUri,
          type: "video/mp4",
          name: `face_scan_${Date.now()}.mp4`,
        } as any);

        let cccdUri = ocrData.cccd_front;

        if (Platform.OS === "android") {
          cccdUri = cccdUri.startsWith("file://")
            ? cccdUri
            : `file://${cccdUri}`;
        } else {
          cccdUri = cccdUri.startsWith("file://")
            ? cccdUri
            : `file://${cccdUri}`;
        }

        console.log("CCCD URI:", cccdUri);

        formData.append("cmnd", {
          uri: cccdUri,
          type: "image/jpeg",
          name: `cmnd_${Date.now()}.jpg`,
        } as any);

        console.log("üöÄ Calling mutation...");
        await faceScanMutation.mutateAsync(formData as any);
        console.log("‚úÖ Mutation successful");

        if (Platform.OS === "android" && finalVideoUri !== videoPath) {
          try {
            await deleteAsync(finalVideoUri, { idempotent: true });
            console.log("üóëÔ∏è Cleaned up temp video file");
          } catch (deleteError) {
            console.warn("‚ö†Ô∏è Could not delete temp file:", deleteError);
          }
        }
      } catch (error) {
        console.error("‚ùå Submit error:", error);

        setCurrentStep("instruction");
              stopRecording();

        Alert.alert(
          "L·ªói x√°c th·ª±c",
          "Kh√¥ng th·ªÉ g·ª≠i video x√°c th·ª±c. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi m·∫°ng v√† th·ª≠ l·∫°i.",
          [
            {
              text: "Th·ª≠ l·∫°i",
              onPress: () => {
                faceScanMutation.reset();
                resetToInitialState();
              },
            },
            {
              text: "H·ªßy",
              style: "cancel",
              onPress: () => {
                faceScanMutation.reset();
                router.back();
              },
            },
          ]
        );
      }
    },
    [user, ocrData, router, faceScanMutation, resetToInitialState]
  );

  const cancelScan = useCallback(() => {
    console.log("‚ùå Canceling...");
    if (isRecordingRef.current) {
      stopRecording();
    }
    cleanupRecording();
    router.back();
  }, [stopRecording, cleanupRecording, router]);

  // ==================== EFFECTS ====================

  useEffect(() => {
    if (faceScanMutation.isSuccess) {
      console.log("‚úÖ Success");
      setCurrentStep("success");
      const timeout = setTimeout(() => {
        router.push("/settings/profile");
      }, 2000);
      return () => clearTimeout(timeout);
    }

    if (faceScanMutation.isError) {
      console.error("‚ùå Error:", faceScanMutation.error);
            stopRecording();

      Alert.alert(
        "L·ªói x√°c th·ª±c",
        faceScanMutation.error?.message ||
          "Kh√¥ng th·ªÉ x√°c th·ª±c khu√¥n m·∫∑t. Vui l√≤ng th·ª≠ l·∫°i.",
        [
          {
            text: "Th·ª≠ l·∫°i",
            onPress: () => {
              faceScanMutation.reset();
              resetToInitialState();
            },
          },
          {
            text: "H·ªßy",
            style: "cancel",
            onPress: () => {
              faceScanMutation.reset();
              router.back();
            },
          },
        ]
      );
    }
  }, [
    faceScanMutation.isSuccess,
    faceScanMutation.isError,
    router,
    resetToInitialState,
  ]);

  useEffect(() => {
    if (Platform.OS === "ios" || Platform.OS === "android") {
      if (!hasPermission) {
        requestPermission();
      }
    }
  }, [hasPermission, requestPermission]);

  useEffect(() => {
    return () => {
      console.log("üßπ Component unmounting");
      cleanupRecording();
    };
  }, [cleanupRecording]);

  // ==================== RENDER COMPONENTS ====================

  // ‚úÖ Component v·∫Ω n·ªÅn tr·∫Øng v·ªõi l·ªó oval ·ªü gi·ªØa
  const OvalMaskOverlay = () => {
    const centerX = SCREEN_WIDTH / 2;
    const centerY = CAMERA_HEIGHT / 2;
    const radiusX = FACE_OVAL_WIDTH / 2;
    const radiusY = FACE_OVAL_HEIGHT / 2;

    return (
      <Svg
        width={SCREEN_WIDTH}
        height={CAMERA_HEIGHT}
        style={StyleSheet.absoluteFillObject}
      >
        <Defs>
          <Mask id="oval-mask">
            {/* N·ªÅn tr·∫Øng to√†n m√†n h√¨nh */}
            <Rect width={SCREEN_WIDTH} height={CAMERA_HEIGHT} fill="white" />
            {/* L·ªó oval ·ªü gi·ªØa (m√†u ƒëen ƒë·ªÉ t·∫°o l·ªó trong mask) */}
            <Ellipse
              cx={centerX}
              cy={centerY}
              rx={radiusX}
              ry={radiusY}
              fill="black"
            />
          </Mask>
        </Defs>
        {/* Apply mask l√™n n·ªÅn tr·∫Øng */}
        <Rect
          width={SCREEN_WIDTH}
          height={CAMERA_HEIGHT}
          fill="white"
          opacity={1}
          mask="url(#oval-mask)"
        />
      </Svg>
    );
  };

  // ‚úÖ Component v·∫Ω v√≤ng loading xung quanh oval
  const OvalLoadingRing = ({ progress }: { progress: number }) => {
    const centerX = SCREEN_WIDTH / 2;
    const centerY = CAMERA_HEIGHT / 2;
    const radiusX = FACE_OVAL_HEIGHT / 2 + RING_GAP;
    const radiusY = FACE_OVAL_WIDTH / 2 + RING_GAP;

    // T√≠nh circumference c·ªßa ellipse (c√¥ng th·ª©c Ramanujan x·∫•p x·ªâ)
    const h = Math.pow(radiusX - radiusY, 2) / Math.pow(radiusX + radiusY, 2);
    const circumference =
      Math.PI *
      (radiusX + radiusY) *
      (1 + (3 * h) / (10 + Math.sqrt(4 - 3 * h)));

    const strokeDashoffset = circumference - (circumference * progress) / 100;

    return (
      <Svg
        width={SCREEN_WIDTH}
        height={CAMERA_HEIGHT}
        style={StyleSheet.absoluteFillObject}
      >
        {/* Background ring (m√†u x√°m m·ªù) */}
        <Ellipse
          cx={centerX}
          cy={centerY}
          rx={radiusY}
          ry={radiusX}
          fill="none"
          stroke="rgba(200,200,200,0.3)"
          strokeWidth={RING_STROKE_WIDTH}
        />
        <Ellipse
          cx={centerX}
          cy={centerY}
          rx={radiusX}
          ry={radiusY}
          fill="none"
          stroke={isPaused ? colors.warning : colors.primary}
          strokeWidth={RING_STROKE_WIDTH}
          strokeDasharray={`${circumference} ${circumference}`}
          strokeDashoffset={strokeDashoffset}
          strokeLinecap="round"
          transform={`rotate(-90 ${centerX} ${centerY})`}
        />
      </Svg>
    );
  };

  // ‚úÖ Component v·∫Ω vi·ªÅn oval
  const OvalBorder = () => {
    const centerX = SCREEN_WIDTH / 2;
    const centerY = CAMERA_HEIGHT / 2;
    const radiusX = FACE_OVAL_WIDTH / 2;
    const radiusY = FACE_OVAL_HEIGHT / 2;

    return (
      <Svg
        width={SCREEN_WIDTH}
        height={CAMERA_HEIGHT}
        style={StyleSheet.absoluteFillObject}
      >
        <Ellipse
          cx={centerX}
          cy={centerY}
          rx={radiusX}
          ry={radiusY}
          fill="none"
          stroke={faceDetected && !isPaused ? colors.success : colors.warning}
          strokeWidth={3}
        />
      </Svg>
    );
  };

  // ==================== RENDER CHECKS ====================

  if (!hasPermission) {
    return (
      <Box
        flex={1}
        bg={colors.background}
        justifyContent="center"
        alignItems="center"
        p="$6"
      >
        <Camera size={64} color={colors.textSecondary} />
        <Text
          fontSize="$lg"
          fontWeight="$bold"
          color={colors.text}
          mt="$4"
          textAlign="center"
        >
          C·∫ßn quy·ªÅn truy c·∫≠p camera
        </Text>
        <Text
          fontSize="$sm"
          color={colors.textSecondary}
          mt="$2"
          textAlign="center"
        >
          Agrisa c·∫ßn quy·ªÅn s·ª≠ d·ª•ng camera ƒë·ªÉ x√°c th·ª±c khu√¥n m·∫∑t c·ªßa b·∫°n
        </Text>
        <Button
          size="lg"
          bg={colors.primary}
          onPress={requestPermission}
          mt="$6"
        >
          <ButtonText color={colors.text}>C·∫•p quy·ªÅn</ButtonText>
        </Button>
      </Box>
    );
  }

  if (!device) {
    return (
      <Box
        flex={1}
        bg={colors.background}
        justifyContent="center"
        alignItems="center"
        p="$6"
      >
        <Spinner size="large" color={colors.primary} />
        <Text mt="$4" color={colors.text}>
          ƒêang kh·ªüi t·∫°o camera...
        </Text>
      </Box>
    );
  }

  // ==================== RENDER SCREENS ====================

  const renderInstructionScreen = () => (
    <Box flex={1} bg={colors.background} justifyContent="center" px="$6">
      <VStack space="2xl" alignItems="center">
        <ScanFace size={80} color={colors.primary} />

        <VStack space="md" alignItems="center">
          <Text
            fontSize="$2xl"
            fontWeight="$bold"
            color={colors.text}
            textAlign="center"
          >
            X√°c th·ª±c khu√¥n m·∫∑t
          </Text>
          <Text fontSize="$sm" color={colors.textSecondary} textAlign="center">
            Qu√° tr√¨nh n√†y s·∫Ω ghi h√¨nh khu√¥n m·∫∑t c·ªßa b·∫°n trong 10 gi√¢y
          </Text>
        </VStack>

        <Box
          bg={colors.surface}
          p="$5"
          borderRadius="$lg"
          borderWidth={1}
          borderColor={colors.border}
          w="$full"
        >
          <VStack space="md">
            <Text fontSize="$sm" fontWeight="$semibold" color={colors.text}>
              L∆∞u √Ω khi quay video:
            </Text>
            <VStack space="sm">
              <Text fontSize="$xs" color={colors.textSecondary}>
                ‚Ä¢ ƒê·∫∑t khu√¥n m·∫∑t v√†o trong khung oval
              </Text>
              <Text fontSize="$xs" color={colors.textSecondary}>
                ‚Ä¢ Nh√¨n th·∫≥ng v√†o camera v√† gi·ªØ nguy√™n t∆∞ th·∫ø
              </Text>
              <Text fontSize="$xs" color={colors.textSecondary}>
                ‚Ä¢ ƒê·∫£m b·∫£o c√≥ ƒë·ªß √°nh s√°ng, tr√°nh ng∆∞·ª£c s√°ng
              </Text>
              <Text fontSize="$xs" color={colors.textSecondary}>
                ‚Ä¢ N·∫øu kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t, qu√° tr√¨nh s·∫Ω t·∫°m d·ª´ng
              </Text>
            </VStack>
          </VStack>
        </Box>

        <Button
          size="lg"
          bg={colors.primary}
          onPress={startPreparation}
          w="$full"
        >
          <ButtonText color={colors.text} fontWeight="$semibold">
            B·∫Øt ƒë·∫ßu quay
          </ButtonText>
        </Button>
      </VStack>
    </Box>
  );

  const renderCameraScreen = () => (
    <Box flex={1} bg={colors.background}>
      {/* Header */}
      <Box
        position="absolute"
        top={0}
        left={0}
        right={0}
        zIndex={20}
        bg="$white"
        p="$4"
      >
        <HStack justifyContent="space-between" alignItems="center">
          <VStack flex={1}>
            <Text fontSize="$lg" fontWeight="$bold" color={colors.text}>
              {currentStep === "preparing"
                ? "Chu·∫©n b·ªã..."
                : "X√°c th·ª±c khu√¥n m·∫∑t"}
            </Text>
            {faceDetectionStatus === "error" && (
              <Text fontSize="$xs" color={colors.warning}>
                C√≥ l·ªói khi quay h√¨nh, h√£y hu·ª∑ v√† b·∫Øt ƒë·∫ßu l·∫°i
              </Text>
            )}
          </VStack>
          <Button size="sm" variant="link" onPress={cancelScan}>
            <ButtonIcon as={X} color={colors.text} />
          </Button>
        </HStack>
      </Box>

      <Box
        position="absolute"
        top={insets.top}
        overflow="hidden"
        left={0}
        right={0}
        bottom={insets.bottom + 60}
      >
        {/* Camera view */}
        <FaceCamera
          ref={cameraRef}
          style={StyleSheet.absoluteFillObject}
          device={device}
          isActive={currentStep === "preparing" || currentStep === "recording"}
          video={true}
          audio={false}
          faceDetectionCallback={handleFacesDetection}
          faceDetectionOptions={faceDetectionOptions}
        />

        {/* N·ªÅn tr·∫Øng v·ªõi l·ªó oval */}
        <OvalMaskOverlay />

        {/* Vi·ªÅn oval */}
        <OvalBorder />

        {/* V√≤ng loading xung quanh oval */}
        {currentStep === "recording" && (
          <OvalLoadingRing progress={recordingProgress} />
        )}

        {/* Status messages ph√≠a tr√™n khung */}
        <Box
          position="absolute"
          top={120}
          left={0}
          right={0}
          alignItems="center"
          px="$6"
        >
          {currentStep === "preparing" &&
            !faceDetected &&
            faceDetectionStatus === "checking" && (
              <Box
                bg="rgba(0,0,0,0.7)"
                borderRadius="$lg"
                p="$4"
                alignItems="center"
              >
                <Spinner size="small" color={colors.primary} />
                <Text fontSize="$sm" color="white" mt="$2" textAlign="center">
                  ƒêang ki·ªÉm tra camera...
                </Text>
              </Box>
            )}

          {currentStep === "preparing" &&
            faceDetectionStatus === "working" &&
            !faceDetected && (
              <Box
                bg="rgba(0,0,0,0.2)"
                borderRadius="$lg"
                p="$4"
                alignItems="center"
              >
                <Spinner size="small" color={colors.primary} />
                <Text fontSize="$sm" color="white" mt="$2" textAlign="center">
                  ƒêang t√¨m khu√¥n m·∫∑t...
                </Text>
              </Box>
            )}
          {currentStep === "recording" && (
            <Box borderRadius="$lg" p="$4" alignItems="center" bg="$red600">
              <HStack space="sm" alignItems="center" px="$4">
                <ButtonIcon
                  as={Video}
                  color={colors.textWhiteButton}
                  size="lg"
                />
                <Text
                  color={colors.textWhiteButton}
                  fontWeight="$bold"
                  fontSize="$md"
                >
                  {isPaused ? "ƒê√£ t·∫°m d·ª´ng" : "ƒêang quay"}
                </Text>
              </HStack>
            </Box>
          )}
        </Box>

        {/* Text h∆∞·ªõng d·∫´n ·ªü gi·ªØa (ph√≠a d∆∞·ªõi khung oval) */}
        <Box
          position="absolute"
          bottom={CAMERA_HEIGHT / 2 - FACE_OVAL_HEIGHT / 2 - 70}
          left={0}
          right={0}
          px="$6"
        >
          <Text
            fontSize="$lg"
            color={colors.text}
            textAlign="center"
            fontWeight="$bold"
            p="$3"
            borderRadius="$lg"
          >
            {currentStep === "preparing" && !faceDetected
              ? "ƒê·∫∑t khu√¥n m·∫∑t v√†o khung"
              : currentStep === "recording"
                ? isPaused
                  ? "Gi·ªØ khu√¥n m·∫∑t trong khung"
                  : "ƒêang qu√©t..."
                : "ƒê·∫∑t khu√¥n m·∫∑t v√†o khung"}
          </Text>
        </Box>
      </Box>
    </Box>
  );

  const renderProcessingScreen = () => (
    <Box
      flex={1}
      bg={colors.background}
      justifyContent="center"
      alignItems="center"
      p="$6"
    >
      <Spinner size="large" color={colors.primary} />
      <Text fontSize="$lg" fontWeight="$semibold" color={colors.text} mt="$4">
        ƒêang x·ª≠ l√Ω video x√°c th·ª±c...
      </Text>
      <Text
        fontSize="$sm"
        color={colors.textSecondary}
        mt="$2"
        textAlign="center"
      >
        H·ªá th·ªëng ƒëang x√°c th·ª±c khu√¥n m·∫∑t c·ªßa b·∫°n
      </Text>
      <Text
        fontSize="$xs"
        color={colors.textSecondary}
        mt="$4"
        textAlign="center"
        px="$6"
      >
        Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i gi√¢y. Vui l√≤ng kh√¥ng t·∫Øt ·ª©ng d·ª•ng.
      </Text>
    </Box>
  );

  const renderSuccessScreen = () => (
    <Box
      flex={1}
      bg={colors.background}
      justifyContent="center"
      alignItems="center"
      p="$6"
    >
      <CheckCircle2 size={80} color={colors.success} />
      <Text
        fontSize="$2xl"
        fontWeight="$bold"
        color={colors.text}
        mt="$4"
        textAlign="center"
      >
        X√°c th·ª±c th√†nh c√¥ng!
      </Text>
      <Text
        fontSize="$sm"
        color={colors.textSecondary}
        mt="$2"
        textAlign="center"
      >
        T√†i kho·∫£n c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c x√°c th·ª±c
      </Text>
    </Box>
  );

  switch (currentStep) {
    case "instruction":
      return renderInstructionScreen();
    case "preparing":
    case "recording":
      return renderCameraScreen();
    case "processing":
      return renderProcessingScreen();
    case "success":
      return renderSuccessScreen();
    default:
      return renderInstructionScreen();
  }
};
