import { useAgrisaColors } from "@/domains/agrisa_theme/hooks/useAgrisaColor";
import { useAuthStore } from "@/domains/auth/stores/auth.store";
import {
  Box,
  Button,
  ButtonIcon,
  ButtonText,
  HStack,
  Progress,
  ProgressFilledTrack,
  Spinner,
  Text,
  VStack,
} from "@gluestack-ui/themed";
import {
  cacheDirectory,
  copyAsync,
  deleteAsync,
  documentDirectory,
} from "expo-file-system/legacy";
import { useRouter } from "expo-router";
import { Camera, CheckCircle2, User, X } from "lucide-react-native";
import React, { useCallback, useEffect, useMemo, useRef, useState } from "react";
import { Alert, Dimensions, Platform, StyleSheet } from "react-native";
import {
  Camera as VisionCamera,
  useCameraDevice,
  useCameraPermission,
} from "react-native-vision-camera";
import {
  Face,
  Camera as FaceCamera,
  FaceDetectionOptions,
} from "react-native-vision-camera-face-detector";
import { useEkyc } from "../hooks/use-ekyc";
import { useEkycStore } from "../stores/ekyc.store";

const { width: SCREEN_WIDTH } = Dimensions.get("window");

// Constants
const FACE_FRAME_WIDTH = SCREEN_WIDTH * 0.7;
const FACE_FRAME_HEIGHT = FACE_FRAME_WIDTH * 1.3;
const CAMERA_HEIGHT = FACE_FRAME_HEIGHT + 200;

const RECORDING_DURATION = 10000; // 10 gi√¢y
const NO_FACE_TIMEOUT = 120000; // 2 ph√∫t
const FACE_DETECTION_INTERVAL = 500; // Ki·ªÉm tra m·ªói 500ms
const MAX_NO_FACE_PAUSE = 3000; // C·∫£nh b√°o sau 3 gi√¢y kh√¥ng c√≥ face

type ScanStep =
  | "instruction"
  | "preparing"
  | "recording"
  | "processing"
  | "success";

export const FaceScanScreen = () => {
  const router = useRouter();
  const { colors } = useAgrisaColors();
  const { faceScanMutation } = useEkyc();
  const { ocrData } = useEkycStore();
  const { user } = useAuthStore();

  // Camera hooks
  const { hasPermission, requestPermission } = useCameraPermission();
  const device = useCameraDevice("front");

  // Refs
  const cameraRef = useRef<VisionCamera>(null);
  const recordingTimeRef = useRef<number>(0);
  const pausedTimeRef = useRef<number>(0);
  const lastFaceDetectedTimeRef = useRef<number>(Date.now());
  const faceDetectionCallCountRef = useRef<number>(0);
  const isFaceDetectionWorkingRef = useRef<boolean>(false);
  const isRecordingRef = useRef<boolean>(false);
  const recordedVideoPathRef = useRef<string | null>(null);
  const currentlyPausedRef = useRef<boolean>(false); // Track pause state trong ref

  // Timeout refs
  const noFaceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const recordingIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const faceDetectionCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const pauseCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // States
  const [currentStep, setCurrentStep] = useState<ScanStep>("instruction");
  const [faceDetected, setFaceDetected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [recordingProgress, setRecordingProgress] = useState(0);
  const [faceDetectionStatus, setFaceDetectionStatus] = useState<
    "working" | "error" | "checking"
  >("checking");

  // Face Detection Options
  const faceDetectionOptions = useMemo<FaceDetectionOptions>(
    () => ({
      performanceMode: "accurate",
      landmarkMode: "all",
      contourMode: "none",
      classificationMode: "all",
      trackingEnabled: true,
      windowWidth: SCREEN_WIDTH,
      windowHeight: CAMERA_HEIGHT,
    }),
    []
  );

  // ==================== CLEANUP FUNCTIONS ====================

  const cleanupRecording = useCallback(() => {
    console.log("üßπ Cleaning up recording...");

    if (recordingIntervalRef.current) {
      clearInterval(recordingIntervalRef.current);
      recordingIntervalRef.current = null;
    }
    if (noFaceTimeoutRef.current) {
      clearTimeout(noFaceTimeoutRef.current);
      noFaceTimeoutRef.current = null;
    }
    if (faceDetectionCheckIntervalRef.current) {
      clearInterval(faceDetectionCheckIntervalRef.current);
      faceDetectionCheckIntervalRef.current = null;
    }
    if (pauseCheckIntervalRef.current) {
      clearInterval(pauseCheckIntervalRef.current);
      pauseCheckIntervalRef.current = null;
    }

    recordingTimeRef.current = 0;
    pausedTimeRef.current = 0;
    isRecordingRef.current = false;
    recordedVideoPathRef.current = null;
    isFaceDetectionWorkingRef.current = false;
    faceDetectionCallCountRef.current = 0;
    currentlyPausedRef.current = false;
  }, []);

  const resetToInitialState = useCallback(() => {
    console.log("üîÑ Resetting to initial state...");
    cleanupRecording();
    setCurrentStep("instruction");
    setFaceDetected(false);
    setIsRecording(false);
    setIsPaused(false);
    setRecordingProgress(0);
    setFaceDetectionStatus("checking");
  }, [cleanupRecording]);

  // ==================== FACE DETECTION HEALTH CHECK ====================

  const startFaceDetectionCheck = useCallback(() => {
    console.log("üîç Starting face detection health check...");

    if (faceDetectionCheckIntervalRef.current) {
      clearInterval(faceDetectionCheckIntervalRef.current);
    }

    let checkCount = 0;
    const maxChecks = 10; // 5 gi√¢y

    faceDetectionCheckIntervalRef.current = setInterval(() => {
      checkCount++;
      console.log(
        `üîç Check ${checkCount}/${maxChecks}, callbacks: ${faceDetectionCallCountRef.current}`
      );

      if (faceDetectionCallCountRef.current >= 3) {
        console.log("‚úÖ Face detection is working!");
        isFaceDetectionWorkingRef.current = true;
        setFaceDetectionStatus("working");

        if (faceDetectionCheckIntervalRef.current) {
          clearInterval(faceDetectionCheckIntervalRef.current);
          faceDetectionCheckIntervalRef.current = null;
        }
        return;
      }

      if (checkCount >= maxChecks) {
        console.warn("‚ö†Ô∏è Face detection NOT working! Using auto mode.");
        isFaceDetectionWorkingRef.current = false;
        setFaceDetectionStatus("error");

        if (faceDetectionCheckIntervalRef.current) {
          clearInterval(faceDetectionCheckIntervalRef.current);
          faceDetectionCheckIntervalRef.current = null;
        }

        Alert.alert(
          "Th√¥ng b√°o",
          "Kh√¥ng th·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng ph√°t hi·ªán khu√¥n m·∫∑t. H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông ghi h√¨nh 10 gi√¢y.\n\nVui l√≤ng gi·ªØ khu√¥n m·∫∑t trong khung trong su·ªët qu√° tr√¨nh.",
          [
            {
              text: "Ti·∫øp t·ª•c",
              onPress: () => {
                setTimeout(() => {
                  startRecording();
                }, 1000);
              },
            },
            {
              text: "H·ªßy",
              style: "cancel",
              onPress: () => resetToInitialState(),
            },
          ]
        );
      }
    }, FACE_DETECTION_INTERVAL);
  }, [resetToInitialState]);

  // ==================== FACE DETECTION CALLBACK ====================

  const handleFacesDetection = useCallback(
    (faces: Face[]) => {
      try {
        faceDetectionCallCountRef.current++;

        const hasValidFace =
          faces?.length > 0 &&
          (() => {
            const face = faces[0];
            return (
              face.leftEyeOpenProbability > 0.7 &&
              face.rightEyeOpenProbability > 0.7 &&
              Math.abs(face.yawAngle) < 15 &&
              Math.abs(face.pitchAngle) < 15
            );
          })();

        setFaceDetected(hasValidFace);

        // CH·ªà x·ª≠ l√Ω khi ƒëang recording v√† face detection ho·∫°t ƒë·ªông
        if (currentStep === "recording" && isFaceDetectionWorkingRef.current) {
          if (hasValidFace) {
            lastFaceDetectedTimeRef.current = Date.now();

            // Resume n·∫øu ƒëang pause
            if (currentlyPausedRef.current) {
              console.log("üòä Face detected, RESUMING...");
              currentlyPausedRef.current = false;
              setIsPaused(false);
              pausedTimeRef.current = 0;
            }
          } else {
            // Pause n·∫øu kh√¥ng c√≥ face
            if (!currentlyPausedRef.current) {
              console.log("üòü No face detected, PAUSING...");
              currentlyPausedRef.current = true;
              setIsPaused(true);
            }
          }
        }
        // N·∫øu ƒëang preparing v√† detect ƒë∆∞·ª£c face -> start recording
        else if (
          currentStep === "preparing" &&
          hasValidFace &&
          isFaceDetectionWorkingRef.current
        ) {
          console.log("üë§ Face detected in preparing, starting recording...");
          startRecording();
        }
      } catch (error) {
        console.error("‚ùå Face detection error:", error);
        setFaceDetected(false);
      }
    },
    [currentStep]
  );

  // ==================== RECORDING FUNCTIONS ====================

  const startPreparation = useCallback(() => {
    console.log("üé¨ Starting preparation...");

    cleanupRecording();
    setFaceDetected(false);
    setIsRecording(false);
    setIsPaused(false);
    setRecordingProgress(0);
    setFaceDetectionStatus("checking");

    setCurrentStep("preparing");
    startFaceDetectionCheck();

    noFaceTimeoutRef.current = setTimeout(() => {
      console.error("‚è∞ No face timeout");
      Alert.alert(
        "H·∫øt th·ªùi gian",
        "Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t sau 2 ph√∫t. Vui l√≤ng th·ª≠ l·∫°i.",
        [
          {
            text: "OK",
            onPress: () => {
              if (isRecordingRef.current) {
                stopRecording();
              }
              resetToInitialState();
            },
          },
        ]
      );
    }, NO_FACE_TIMEOUT);
  }, [cleanupRecording, startFaceDetectionCheck, resetToInitialState]);

  const startRecording = useCallback(async () => {
    if (!cameraRef.current || isRecordingRef.current) {
      console.log("‚ö†Ô∏è Cannot start recording");
      return;
    }

    try {
      console.log("üé• Starting recording...");

      setCurrentStep("recording");
      setIsRecording(true);
      isRecordingRef.current = true;
      currentlyPausedRef.current = false; // Reset pause state
      setIsPaused(false);
      recordingTimeRef.current = 0;
      pausedTimeRef.current = 0;
      setRecordingProgress(0);

      await cameraRef.current.startRecording({
        onRecordingFinished: (video) => {
          console.log("‚úÖ Recording finished:", video.path);
          recordedVideoPathRef.current = video.path;
          isRecordingRef.current = false;
          setIsRecording(false);

          if (recordingTimeRef.current >= RECORDING_DURATION) {
            handleSubmit(video.path);
          }
        },
        onRecordingError: (error) => {
          console.error("‚ùå Recording error:", error);
          isRecordingRef.current = false;
          setIsRecording(false);

          Alert.alert("L·ªói", "Kh√¥ng th·ªÉ ghi h√¨nh. Vui l√≤ng th·ª≠ l·∫°i.", [
            {
              text: "Th·ª≠ l·∫°i",
              onPress: () => resetToInitialState(),
            },
          ]);
        },
      });

      if (recordingIntervalRef.current) {
        clearInterval(recordingIntervalRef.current);
      }

      // Interval c·∫≠p nh·∫≠t progress - CH·ªà tƒÉng khi KH√îNG pause
      recordingIntervalRef.current = setInterval(() => {
        const shouldPause =
          isFaceDetectionWorkingRef.current && currentlyPausedRef.current;

        if (!shouldPause) {
          // TƒÉng recording time
          recordingTimeRef.current += 100;

          const progress = Math.min(
            (recordingTimeRef.current / RECORDING_DURATION) * 100,
            100
          );
          setRecordingProgress(progress);

          console.log(
            `üìä Recording: ${recordingTimeRef.current}ms (${progress.toFixed(1)}%) - Paused: ${shouldPause}`
          );

          if (recordingTimeRef.current >= RECORDING_DURATION) {
            console.log("‚úÖ Recording duration reached");
            stopRecording();
          }
        } else {
          // TƒÉng paused time
          pausedTimeRef.current += 100;
          console.log(`‚è∏Ô∏è Paused: ${pausedTimeRef.current}ms`);
        }
      }, 100);

      // Interval ki·ªÉm tra pause qu√° l√¢u
      if (isFaceDetectionWorkingRef.current) {
        if (pauseCheckIntervalRef.current) {
          clearInterval(pauseCheckIntervalRef.current);
        }

        pauseCheckIntervalRef.current = setInterval(() => {
          if (currentlyPausedRef.current) {
            const timeSinceLastFace =
              Date.now() - lastFaceDetectedTimeRef.current;

            if (timeSinceLastFace > MAX_NO_FACE_PAUSE) {
              console.warn("‚ö†Ô∏è Paused too long");

              if (pauseCheckIntervalRef.current) {
                clearInterval(pauseCheckIntervalRef.current);
                pauseCheckIntervalRef.current = null;
              }

              Alert.alert(
                "C·∫£nh b√°o",
                "Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t trong 3 gi√¢y. Vui l√≤ng ƒë∆∞a khu√¥n m·∫∑t v√†o khung.",
                [
                  {
                    text: "Ti·∫øp t·ª•c",
                    onPress: () => {
                      lastFaceDetectedTimeRef.current = Date.now();
                      pausedTimeRef.current = 0;
                    },
                  },
                  {
                    text: "B·∫Øt ƒë·∫ßu l·∫°i",
                    onPress: () => {
                      stopRecording();
                      resetToInitialState();
                    },
                  },
                ]
              );
            }
          }
        }, 1000);
      }
    } catch (error) {
      console.error("‚ùå Start recording error:", error);
      isRecordingRef.current = false;
      setIsRecording(false);

      Alert.alert("L·ªói", "Kh√¥ng th·ªÉ b·∫Øt ƒë·∫ßu ghi h√¨nh. Vui l√≤ng th·ª≠ l·∫°i.", [
        {
          text: "Th·ª≠ l·∫°i",
          onPress: () => resetToInitialState(),
        },
      ]);
    }
  }, [resetToInitialState]);

  const stopRecording = useCallback(async () => {
    if (!cameraRef.current || !isRecordingRef.current) {
      console.log("‚ö†Ô∏è Already stopped or not recording");
      return;
    }

    try {
      console.log("üõë Stopping recording...");

      if (recordingIntervalRef.current) {
        clearInterval(recordingIntervalRef.current);
        recordingIntervalRef.current = null;
      }
      if (noFaceTimeoutRef.current) {
        clearTimeout(noFaceTimeoutRef.current);
        noFaceTimeoutRef.current = null;
      }
      if (pauseCheckIntervalRef.current) {
        clearInterval(pauseCheckIntervalRef.current);
        pauseCheckIntervalRef.current = null;
      }

      await cameraRef.current.stopRecording();
      console.log("‚úÖ Recording stopped");
    } catch (error) {
      console.error("‚ùå Stop recording error:", error);
      isRecordingRef.current = false;
      setIsRecording(false);
    }
  }, []);

  // ==================== SUBMIT ====================

  const handleSubmit = useCallback(
    async (videoPath: string) => {
      console.log("üì§ Submitting video...");
      console.log("Original video path:", videoPath);

      if (!user?.id || !ocrData.cccd_front) {
        Alert.alert(
          "L·ªói",
          "Thi·∫øu th√¥ng tin x√°c th·ª±c. Vui l√≤ng quay l·∫°i v√† th·ª±c hi·ªán l·∫°i t·ª´ ƒë·∫ßu.",
          [{ text: "ƒê√≥ng", onPress: () => router.back() }]
        );
        return;
      }

      setCurrentStep("processing");

      try {
        const formData = new FormData();
        formData.append("user_id", user.id.toString());

        // ‚úÖ FIX: S·ª≠ d·ª•ng legacy API - ·ªïn ƒë·ªãnh v√† d·ªÖ maintain
        let finalVideoUri = videoPath;

        if (Platform.OS === "android") {
          // Android: Copy file sang document directory ƒë·ªÉ c√≥ ƒë∆∞·ªùng d·∫´n stable
          const fileName = `face_scan_${Date.now()}.mp4`;

          // ‚úÖ S·ª≠ d·ª•ng documentDirectory t·ª´ legacy import
          if (!documentDirectory) {
            console.warn("‚ö†Ô∏è documentDirectory not available, using cache");
            const newPath = `${cacheDirectory}${fileName}`;

            try {
              // ‚úÖ S·ª≠ d·ª•ng copyAsync t·ª´ legacy import
              await copyAsync({
                from: videoPath,
                to: newPath,
              });

              console.log("‚úÖ Video copied to cache:", newPath);
              finalVideoUri = newPath;
            } catch (copyError) {
              console.error("‚ùå Error copying video:", copyError);
              // Fallback: S·ª≠ d·ª•ng path g·ªëc
              finalVideoUri = videoPath.startsWith("file://")
                ? videoPath
                : `file://${videoPath}`;
            }
          } else {
            const newPath = `${documentDirectory}${fileName}`;

            try {
              // ‚úÖ S·ª≠ d·ª•ng copyAsync t·ª´ legacy import
              await copyAsync({
                from: videoPath,
                to: newPath,
              });

              console.log("‚úÖ Video copied to:", newPath);
              finalVideoUri = newPath;
            } catch (copyError) {
              console.error("‚ùå Error copying video:", copyError);
              // Fallback: S·ª≠ d·ª•ng path g·ªëc
              finalVideoUri = videoPath.startsWith("file://")
                ? videoPath
                : `file://${videoPath}`;
            }
          }
        } else {
          // iOS: ƒê·∫£m b·∫£o c√≥ file:// prefix
          finalVideoUri = videoPath.startsWith("file://")
            ? videoPath
            : `file://${videoPath}`;
        }

        console.log("Final video URI:", finalVideoUri);

        // Append video v√†o FormData
        formData.append("video", {
          uri: finalVideoUri,
          type: "video/mp4",
          name: `face_scan_${Date.now()}.mp4`,
        } as any);

        // ‚úÖ FIX: Chu·∫©n h√≥a CCCD path
        let cccdUri = ocrData.cccd_front;

        if (Platform.OS === "android") {
          cccdUri = cccdUri.startsWith("file://")
            ? cccdUri
            : `file://${cccdUri}`;
        } else {
          cccdUri = cccdUri.startsWith("file://")
            ? cccdUri
            : `file://${cccdUri}`;
        }

        console.log("CCCD URI:", cccdUri);

        formData.append("cmnd", {
          uri: cccdUri,
          type: "image/jpeg",
          name: `cmnd_${Date.now()}.jpg`,
        } as any);

        console.log("üöÄ Calling mutation...");
        await faceScanMutation.mutateAsync(formData as any);
        console.log("‚úÖ Mutation successful");

        // ‚úÖ Cleanup: X√≥a file t·∫°m n·∫øu ƒë√£ copy - d√πng deleteAsync t·ª´ legacy
        if (Platform.OS === "android" && finalVideoUri !== videoPath) {
          try {
            await deleteAsync(finalVideoUri, { idempotent: true });
            console.log("üóëÔ∏è Cleaned up temp video file");
          } catch (deleteError) {
            console.warn("‚ö†Ô∏è Could not delete temp file:", deleteError);
          }
        }
      } catch (error) {
        console.error("‚ùå Submit error:", error);

        // Reset step ƒë·ªÉ user c√≥ th·ªÉ th·ª≠ l·∫°i
        setCurrentStep("instruction");

        Alert.alert(
          "L·ªói x√°c th·ª±c",
          "Kh√¥ng th·ªÉ g·ª≠i video x√°c th·ª±c. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi m·∫°ng v√† th·ª≠ l·∫°i.",
          [
            {
              text: "Th·ª≠ l·∫°i",
              onPress: () => {
                faceScanMutation.reset();
                resetToInitialState();
              },
            },
            {
              text: "H·ªßy",
              style: "cancel",
              onPress: () => {
                faceScanMutation.reset();
                router.back();
              },
            },
          ]
        );
      }
    },
    [user, ocrData, router, faceScanMutation, resetToInitialState]
  );

  const cancelScan = useCallback(() => {
    console.log("‚ùå Canceling...");
    if (isRecordingRef.current) {
      stopRecording();
    }
    cleanupRecording();
    router.back();
  }, [stopRecording, cleanupRecording, router]);

  // ==================== EFFECTS ====================

  useEffect(() => {
    if (faceScanMutation.isSuccess) {
      console.log("‚úÖ Success");
      setCurrentStep("success");
      const timeout = setTimeout(() => {
        router.push("/settings/profile");
      }, 2000);
      return () => clearTimeout(timeout);
    }

    if (faceScanMutation.isError) {
      console.error("‚ùå Error:", faceScanMutation.error);
      Alert.alert(
        "L·ªói x√°c th·ª±c",
        faceScanMutation.error?.message ||
          "Kh√¥ng th·ªÉ x√°c th·ª±c khu√¥n m·∫∑t. Vui l√≤ng th·ª≠ l·∫°i.",
        [
          {
            text: "Th·ª≠ l·∫°i",
            onPress: () => {
              faceScanMutation.reset();
              resetToInitialState();
            },
          },
          {
            text: "H·ªßy",
            style: "cancel",
            onPress: () => {
              faceScanMutation.reset();
              router.back();
            },
          },
        ]
      );
    }
  }, [
    faceScanMutation.isSuccess,
    faceScanMutation.isError,
    router,
    resetToInitialState,
  ]);

  useEffect(() => {
    if (Platform.OS === "ios" || Platform.OS === "android") {
      if (!hasPermission) {
        requestPermission();
      }
    }
  }, [hasPermission, requestPermission]);

  useEffect(() => {
    return () => {
      console.log("üßπ Component unmounting");
      cleanupRecording();
    };
  }, [cleanupRecording]);

  // ==================== RENDER CHECKS ====================

  if (!hasPermission) {
    return (
      <Box
        flex={1}
        bg={colors.background}
        justifyContent="center"
        alignItems="center"
        p="$6"
      >
        <Camera size={64} color={colors.textSecondary} />
        <Text
          fontSize="$lg"
          fontWeight="$bold"
          color={colors.text}
          mt="$4"
          textAlign="center"
        >
          C·∫ßn quy·ªÅn truy c·∫≠p camera
        </Text>
        <Text
          fontSize="$sm"
          color={colors.textSecondary}
          mt="$2"
          textAlign="center"
        >
          Agrisa c·∫ßn quy·ªÅn s·ª≠ d·ª•ng camera ƒë·ªÉ x√°c th·ª±c khu√¥n m·∫∑t c·ªßa b·∫°n
        </Text>
        <Button
          size="lg"
          bg={colors.primary}
          onPress={requestPermission}
          mt="$6"
        >
          <ButtonText color={colors.text}>C·∫•p quy·ªÅn</ButtonText>
        </Button>
      </Box>
    );
  }

  if (!device) {
    return (
      <Box
        flex={1}
        bg={colors.background}
        justifyContent="center"
        alignItems="center"
        p="$6"
      >
        <Spinner size="large" color={colors.primary} />
        <Text mt="$4" color={colors.text}>
          ƒêang kh·ªüi t·∫°o camera...
        </Text>
      </Box>
    );
  }

  // ==================== RENDER SCREENS ====================

  const renderInstructionScreen = () => (
    <Box flex={1} bg={colors.background} justifyContent="center" p="$6">
      <VStack space="xl" alignItems="center">
        <User size={80} color={colors.primary} />

        <VStack space="md" alignItems="center">
          <Text
            fontSize="$2xl"
            fontWeight="$bold"
            color={colors.text}
            textAlign="center"
          >
            X√°c th·ª±c khu√¥n m·∫∑t
          </Text>
          <Text fontSize="$sm" color={colors.textSecondary} textAlign="center">
            Qu√° tr√¨nh n√†y s·∫Ω ghi h√¨nh khu√¥n m·∫∑t c·ªßa b·∫°n trong 10 gi√¢y
          </Text>
        </VStack>

        <Box
          bg={colors.surface}
          p="$5"
          borderRadius="$lg"
          borderWidth={1}
          borderColor={colors.border}
          w="$full"
        >
          <VStack space="md">
            <Text fontSize="$sm" fontWeight="$semibold" color={colors.text}>
              L∆∞u √Ω khi quay video:
            </Text>
            <VStack space="sm">
              <HStack space="sm" alignItems="flex-start">
                <Text color={colors.primary}>‚Ä¢</Text>
                <Text fontSize="$xs" color={colors.textSecondary} flex={1}>
                  ƒê·∫∑t khu√¥n m·∫∑t v√†o trong khung oval
                </Text>
              </HStack>
              <HStack space="sm" alignItems="flex-start">
                <Text color={colors.primary}>‚Ä¢</Text>
                <Text fontSize="$xs" color={colors.textSecondary} flex={1}>
                  Nh√¨n th·∫≥ng v√†o camera v√† gi·ªØ nguy√™n t∆∞ th·∫ø
                </Text>
              </HStack>
              <HStack space="sm" alignItems="flex-start">
                <Text color={colors.primary}>‚Ä¢</Text>
                <Text fontSize="$xs" color={colors.textSecondary} flex={1}>
                  ƒê·∫£m b·∫£o c√≥ ƒë·ªß √°nh s√°ng, tr√°nh ng∆∞·ª£c s√°ng
                </Text>
              </HStack>
              <HStack space="sm" alignItems="flex-start">
                <Text color={colors.primary}>‚Ä¢</Text>
                <Text fontSize="$xs" color={colors.textSecondary} flex={1}>
                  Video s·∫Ω ghi ƒë·ªß 10 gi√¢y khi ph√°t hi·ªán khu√¥n m·∫∑t
                </Text>
              </HStack>
              <HStack space="sm" alignItems="flex-start">
                <Text color={colors.primary}>‚Ä¢</Text>
                <Text fontSize="$xs" color={colors.textSecondary} flex={1}>
                  N·∫øu m·∫•t khu√¥n m·∫∑t, qu√° tr√¨nh s·∫Ω t·∫°m d·ª´ng
                </Text>
              </HStack>
            </VStack>
          </VStack>
        </Box>

        <Button
          size="lg"
          bg={colors.primary}
          onPress={startPreparation}
          w="$full"
        >
          <ButtonText color={colors.text} fontWeight="$semibold">
            B·∫Øt ƒë·∫ßu quay
          </ButtonText>
        </Button>

        <Button
          size="lg"
          variant="outline"
          borderColor={colors.border}
          onPress={cancelScan}
          w="$full"
        >
          <ButtonText color={colors.text}>H·ªßy</ButtonText>
        </Button>
      </VStack>
    </Box>
  );

  const renderCameraScreen = () => (
    <Box flex={1} bg={colors.background}>
      <Box bg="rgba(0,0,0,0.9)" p="$4" pt="$12">
        <HStack justifyContent="space-between" alignItems="center">
          <VStack>
            <Text fontSize="$lg" fontWeight="$bold" color={colors.text}>
              {currentStep === "preparing" ? "Chu·∫©n b·ªã..." : "ƒêang quay video"}
            </Text>
            {faceDetectionStatus === "error" && (
              <Text fontSize="$xs" color={colors.warning}>
                ‚ö†Ô∏è Ch·∫ø ƒë·ªô t·ª± ƒë·ªông
              </Text>
            )}
            {faceDetectionStatus === "working" && isPaused && (
              <Text fontSize="$xs" color={colors.warning}>
                ‚è∏Ô∏è ƒê√£ t·∫°m d·ª´ng
              </Text>
            )}
          </VStack>
          <Button size="sm" variant="link" onPress={cancelScan}>
            <ButtonIcon as={X} color={colors.text} />
          </Button>
        </HStack>
      </Box>

      <Box
        height={CAMERA_HEIGHT}
        width={SCREEN_WIDTH}
        position="relative"
        overflow="hidden"
      >
        <FaceCamera
          ref={cameraRef}
          style={StyleSheet.absoluteFillObject}
          device={device}
          isActive={currentStep === "preparing" || currentStep === "recording"}
          video={true}
          audio={false}
          faceDetectionCallback={handleFacesDetection}
          faceDetectionOptions={faceDetectionOptions}
        />

        <Box
          flex={1}
          justifyContent="center"
          alignItems="center"
          bg="rgba(0,0,0,0.3)"
        >
          {currentStep === "preparing" &&
            !faceDetected &&
            faceDetectionStatus === "checking" && (
              <Box
                position="absolute"
                zIndex={10}
                bg="rgba(0,0,0,0.8)"
                borderRadius="$lg"
                p="$6"
                alignItems="center"
              >
                <Spinner size="large" color={colors.primary} />
                <Text
                  fontSize="$md"
                  color={colors.text}
                  mt="$4"
                  textAlign="center"
                >
                  ƒêang ki·ªÉm tra camera...
                </Text>
              </Box>
            )}

          {currentStep === "preparing" &&
            faceDetectionStatus === "working" &&
            !faceDetected && (
              <Box
                position="absolute"
                zIndex={10}
                bg="rgba(0,0,0,0.8)"
                borderRadius="$lg"
                p="$6"
                alignItems="center"
              >
                <Spinner size="large" color={colors.primary} />
                <Text
                  fontSize="$md"
                  color={colors.text}
                  mt="$4"
                  textAlign="center"
                >
                  ƒêang t√¨m khu√¥n m·∫∑t...
                </Text>
              </Box>
            )}

          <Box mb="$4" px="$6">
            <Text
              fontSize="$md"
              color={colors.text}
              textAlign="center"
              fontWeight="$semibold"
            >
              {faceDetectionStatus === "error"
                ? "Vui l√≤ng gi·ªØ khu√¥n m·∫∑t trong khung"
                : !faceDetected
                  ? "ƒê∆∞a khu√¥n m·∫∑t v√†o trong khung"
                  : isPaused
                    ? "Gi·ªØ khu√¥n m·∫∑t trong khung"
                    : "Tuy·ªát v·ªùi! ƒêang ghi h√¨nh..."}
            </Text>
          </Box>

          <Box
            width={FACE_FRAME_WIDTH}
            height={FACE_FRAME_HEIGHT}
            borderWidth={4}
            borderColor={
              faceDetected && !isPaused ? colors.success : colors.warning
            }
            borderRadius={999}
            position="relative"
          >
            <Box
              position="absolute"
              top={20}
              left={20}
              width={30}
              height={30}
              borderTopWidth={5}
              borderLeftWidth={5}
              borderColor={
                faceDetected && !isPaused ? colors.success : colors.warning
              }
              borderTopLeftRadius="$full"
            />
            <Box
              position="absolute"
              top={20}
              right={20}
              width={30}
              height={30}
              borderTopWidth={5}
              borderRightWidth={5}
              borderColor={
                faceDetected && !isPaused ? colors.success : colors.warning
              }
              borderTopRightRadius="$full"
            />
            <Box
              position="absolute"
              bottom={20}
              left={20}
              width={30}
              height={30}
              borderBottomWidth={5}
              borderLeftWidth={5}
              borderColor={
                faceDetected && !isPaused ? colors.success : colors.warning
              }
              borderBottomLeftRadius="$full"
            />
            <Box
              position="absolute"
              bottom={20}
              right={20}
              width={30}
              height={30}
              borderBottomWidth={5}
              borderRightWidth={5}
              borderColor={
                faceDetected && !isPaused ? colors.success : colors.warning
              }
              borderBottomRightRadius="$full"
            />
          </Box>

          {isPaused && currentStep === "recording" && (
            <Box
              mt="$4"
              bg="rgba(255,193,7,0.9)"
              px="$4"
              py="$2"
              borderRadius="$md"
            >
              <Text fontSize="$sm" color={colors.background} fontWeight="$bold">
                ‚è∏ T·∫°m d·ª´ng - Vui l√≤ng gi·ªØ khu√¥n m·∫∑t trong khung
              </Text>
            </Box>
          )}
        </Box>
      </Box>

      {currentStep === "recording" && (
        <Box px="$6" py="$4" bg={colors.background}>
          <VStack space="md">
            <HStack justifyContent="space-between" alignItems="center">
              <Text fontSize="$sm" color={colors.text}>
                Ti·∫øn tr√¨nh: {Math.round(recordingProgress)}%
              </Text>
              <Text fontSize="$sm" color={colors.text}>
                {Math.round((recordingProgress / 100) * 10)}s / 10s
              </Text>
            </HStack>
            <Progress value={recordingProgress} size="md">
              <ProgressFilledTrack
                bg={isPaused ? colors.warning : colors.success}
              />
            </Progress>
            <Text
              fontSize="$xs"
              color={colors.textSecondary}
              textAlign="center"
            >
              {isPaused
                ? "ƒêang t·∫°m d·ª´ng - Vui l√≤ng gi·ªØ m·∫∑t trong khung"
                : recordingProgress >= 99
                  ? "ƒêang x·ª≠ l√Ω video..."
                  : "ƒêang ghi h√¨nh - Gi·ªØ nguy√™n t∆∞ th·∫ø"}
            </Text>
            {pausedTimeRef.current > 0 && isPaused && (
              <Text fontSize="$xs" color={colors.warning} textAlign="center">
                ƒê√£ t·∫°m d·ª´ng: {Math.round(pausedTimeRef.current / 1000)}s
              </Text>
            )}
          </VStack>
        </Box>
      )}

      {currentStep === "preparing" && (
        <Box px="$6" py="$4" bg={colors.background}>
          <Text fontSize="$sm" color={colors.text} textAlign="center">
            {faceDetectionStatus === "checking"
              ? "ƒêang ki·ªÉm tra camera..."
              : faceDetectionStatus === "error"
                ? "S·∫µn s√†ng ghi h√¨nh t·ª± ƒë·ªông"
                : "ƒê∆∞a khu√¥n m·∫∑t v√†o khung ƒë·ªÉ b·∫Øt ƒë·∫ßu"}
          </Text>
        </Box>
      )}
    </Box>
  );

  const renderProcessingScreen = () => (
    <Box
      flex={1}
      bg={colors.background}
      justifyContent="center"
      alignItems="center"
      p="$6"
    >
      <Spinner size="large" color={colors.primary} />
      <Text fontSize="$lg" fontWeight="$semibold" color={colors.text} mt="$4">
        ƒêang x·ª≠ l√Ω video x√°c th·ª±c...
      </Text>
      <Text
        fontSize="$sm"
        color={colors.textSecondary}
        mt="$2"
        textAlign="center"
      >
        H·ªá th·ªëng ƒëang x√°c th·ª±c khu√¥n m·∫∑t c·ªßa b·∫°n
      </Text>
      <Text
        fontSize="$xs"
        color={colors.textSecondary}
        mt="$4"
        textAlign="center"
        px="$6"
      >
        Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i gi√¢y. Vui l√≤ng kh√¥ng t·∫Øt ·ª©ng d·ª•ng.
      </Text>
    </Box>
  );

  const renderSuccessScreen = () => (
    <Box
      flex={1}
      bg={colors.background}
      justifyContent="center"
      alignItems="center"
      p="$6"
    >
      <CheckCircle2 size={80} color={colors.success} />
      <Text
        fontSize="$2xl"
        fontWeight="$bold"
        color={colors.text}
        mt="$4"
        textAlign="center"
      >
        X√°c th·ª±c th√†nh c√¥ng!
      </Text>
      <Text
        fontSize="$sm"
        color={colors.textSecondary}
        mt="$2"
        textAlign="center"
      >
        T√†i kho·∫£n c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c x√°c th·ª±c
      </Text>
    </Box>
  );

  switch (currentStep) {
    case "instruction":
      return renderInstructionScreen();
    case "preparing":
    case "recording":
      return renderCameraScreen();
    case "processing":
      return renderProcessingScreen();
    case "success":
      return renderSuccessScreen();
    default:
      return renderInstructionScreen();
  }
};
